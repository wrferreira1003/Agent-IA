#!/usr/bin/env python3
"""
Teste de integraÃ§Ã£o completa entre o agente e as ferramentas do Notion.
Verifica se o modelo Gemini estÃ¡ usando as ferramentas corretamente.
"""

import sys
import asyncio
sys.path.append('.')

from app.agents.main_agent import ChatAgent

async def test_agent_tool_integration():
    """Testa se o agente consegue usar as ferramentas do Notion"""
    print("ğŸ¤– TESTE DE INTEGRAÃ‡ÃƒO: AGENTE + FERRAMENTAS NOTION")
    print("=" * 60)
    
    try:
        # Inicializa o agente
        print("ğŸš€ Inicializando agente...")
        agent = ChatAgent()
        print("âœ… Agente inicializado com sucesso")
        
        # Lista de testes com diferentes tipos de perguntas
        test_cases = [
            {
                "name": "Consulta direta ao Notion",
                "prompt": "O que vocÃª tem no Notion? Me mostre o conteÃºdo disponÃ­vel.",
                "expect_tool": "search_notion_pages"
            },
            {
                "name": "Pergunta sobre documentaÃ§Ã£o",
                "prompt": "Quais informaÃ§Ãµes existem sobre REST API com Django Ninja?",
                "expect_tool": "search_notion_pages"
            },
            {
                "name": "CriaÃ§Ã£o de pÃ¡gina",
                "prompt": "Crie uma pÃ¡gina no Notion com tÃ­tulo 'Teste de IntegraÃ§Ã£o' e conteÃºdo 'Esta pÃ¡gina foi criada pelo teste de integraÃ§Ã£o do agente.'",
                "expect_tool": "create_notion_page"
            },
            {
                "name": "Pergunta sem ferramenta",
                "prompt": "Qual Ã© a capital do Brasil?",
                "expect_tool": None
            }
        ]
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ“‹ Teste {i}/{len(test_cases)}: {test_case['name']}")
            print("-" * 50)
            print(f"Prompt: {test_case['prompt']}")
            
            try:
                # Executa o teste
                response = await agent.get_response(test_case['prompt'])
                
                # Analisa o resultado
                used_tool = None
                if "search_notion_pages" in response.lower() or "pÃ¡gina do notion" in response.lower():
                    used_tool = "search_notion_pages"
                elif "create_notion_page" in response.lower() or "pÃ¡gina criada" in response.lower():
                    used_tool = "create_notion_page"
                
                # Verifica se a ferramenta esperada foi usada
                tool_correct = (test_case['expect_tool'] == used_tool)
                
                result = {
                    "test": test_case['name'],
                    "success": True,
                    "tool_used": used_tool,
                    "tool_expected": test_case['expect_tool'],
                    "tool_correct": tool_correct,
                    "response_length": len(response),
                    "error": None
                }
                
                print(f"âœ… Resposta recebida: {len(response)} caracteres")
                print(f"ğŸ”§ Ferramenta usada: {used_tool or 'Nenhuma'}")
                print(f"ğŸ¯ Ferramenta esperada: {test_case['expect_tool'] or 'Nenhuma'}")
                print(f"âœ“ Correto: {'âœ… Sim' if tool_correct else 'âŒ NÃ£o'}")
                
                # Mostra uma prÃ©via da resposta
                preview = response[:200] + "..." if len(response) > 200 else response
                print(f"ğŸ“ Preview: {preview}")
                
            except Exception as e:
                result = {
                    "test": test_case['name'],
                    "success": False,
                    "tool_used": None,
                    "tool_expected": test_case['expect_tool'],
                    "tool_correct": False,
                    "response_length": 0,
                    "error": str(e)
                }
                print(f"âŒ Erro: {e}")
            
            results.append(result)
        
        # AnÃ¡lise dos resultados
        print("\n" + "=" * 60)
        print("ğŸ“Š ANÃLISE DOS RESULTADOS")
        print("=" * 60)
        
        successful_tests = sum(1 for r in results if r['success'])
        correct_tools = sum(1 for r in results if r['tool_correct'])
        
        print(f"ğŸ“ˆ Testes executados: {len(results)}")
        print(f"âœ… Testes bem-sucedidos: {successful_tests}/{len(results)}")
        print(f"ğŸ”§ Ferramentas corretas: {correct_tools}/{len(results)}")
        
        # Detalhes por teste
        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            tool_status = "ğŸ¯" if result['tool_correct'] else "âŒ"
            print(f"  {status} {tool_status} {result['test']}")
            if result['error']:
                print(f"      Erro: {result['error']}")
        
        # ConclusÃ£o
        success_rate = successful_tests / len(results) * 100
        tool_accuracy = correct_tools / len(results) * 100
        
        print(f"\nğŸ“Š Taxa de sucesso: {success_rate:.1f}%")
        print(f"ğŸ¯ PrecisÃ£o das ferramentas: {tool_accuracy:.1f}%")
        
        if success_rate >= 75 and tool_accuracy >= 50:
            print("\nğŸ‰ INTEGRAÃ‡ÃƒO FUNCIONANDO BEM!")
            return True
        else:
            print("\nâš ï¸ INTEGRAÃ‡ÃƒO PRECISA DE AJUSTES")
            return False
            
    except Exception as e:
        print(f"âŒ Erro crÃ­tico no teste: {e}")
        return False

async def test_direct_model_tools():
    """Testa se o modelo tem as ferramentas registradas corretamente"""
    print("\nğŸ”§ TESTE DE REGISTRO DAS FERRAMENTAS")
    print("=" * 60)
    
    try:
        from app.agents.models.gemini import GeminiModel
        
        model = GeminiModel()
        
        print(f"ğŸ”§ NÃºmero de ferramentas registradas: {len(model.tools)}")
        
        for i, tool in enumerate(model.tools, 1):
            print(f"  {i}. {tool.__name__}")
            print(f"     DescriÃ§Ã£o: {tool.__doc__}")
        
        # Verifica se as ferramentas esperadas estÃ£o presentes
        tool_names = [tool.__name__ for tool in model.tools]
        expected_tools = ['create_notion_page', 'search_notion_pages']
        
        missing_tools = [t for t in expected_tools if t not in tool_names]
        extra_tools = [t for t in tool_names if t not in expected_tools]
        
        print(f"\nâœ… Ferramentas esperadas presentes: {len(expected_tools) - len(missing_tools)}/{len(expected_tools)}")
        
        if missing_tools:
            print(f"âŒ Ferramentas faltando: {missing_tools}")
        
        if extra_tools:
            print(f"â„¹ï¸ Ferramentas extras: {extra_tools}")
        
        return len(missing_tools) == 0
        
    except Exception as e:
        print(f"âŒ Erro ao verificar ferramentas: {e}")
        return False

async def main():
    """Executa todos os testes de integraÃ§Ã£o"""
    print("ğŸš€ SUITE COMPLETA DE TESTES DE INTEGRAÃ‡ÃƒO")
    print("=" * 80)
    
    # Teste 1: Registro das ferramentas
    tools_ok = await test_direct_model_tools()
    
    # Teste 2: IntegraÃ§Ã£o completa
    integration_ok = await test_agent_tool_integration()
    
    # Resultado final
    print("\n" + "=" * 80)
    print("ğŸ RESULTADO FINAL")
    print("=" * 80)
    print(f"ğŸ”§ Registro de ferramentas: {'âœ… OK' if tools_ok else 'âŒ FALHA'}")
    print(f"ğŸ¤– IntegraÃ§Ã£o do agente: {'âœ… OK' if integration_ok else 'âŒ FALHA'}")
    
    if tools_ok and integration_ok:
        print("\nğŸ‰ SISTEMA TOTALMENTE INTEGRADO E FUNCIONANDO!")
        print("\nğŸ’¡ O modelo Gemini estÃ¡ usando as ferramentas do Notion corretamente!")
    else:
        print("\nâš ï¸ SISTEMA PRECISA DE CORREÃ‡Ã•ES")
        
        if not tools_ok:
            print("   - Problema no registro das ferramentas")
        if not integration_ok:
            print("   - Problema na integraÃ§Ã£o do agente")

if __name__ == "__main__":
    asyncio.run(main())