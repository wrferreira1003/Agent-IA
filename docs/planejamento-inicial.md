## ğŸ“† Planejamento Detalhado de Desenvolvimento do ServiÃ§o de IA com SDK do Google

### ğŸŒŸ VisÃ£o Geral do Projeto

Criaremos um serviÃ§o de IA autÃ´nomo e modular utilizando o SDK oficial do Google (Google Generative AI), com base em FastAPI (Python) e PostgreSQL, integrando memÃ³ria persistente e contexto via documentos do Notion. O projeto Ã© arquitetado desde o inÃ­cio para permitir:

* IntegraÃ§Ã£o futura com backend Go ou frontend web/mobile.
* Suporte a mÃºltiplos modelos LLM (Gemini, OpenAI, Mistral, etc.).
* ExperiÃªncia de onboarding com agente/personagem virtual.

---

### ğŸ’¡ Fase 1: CriaÃ§Ã£o do Projeto e Ambiente

**Objetivo:** Estabelecer a base do projeto com estrutura de diretÃ³rios, ambiente Python e containerizaÃ§Ã£o.

#### Etapas:

1. Criar diretÃ³rio raiz do projeto:

   ```bash
   mkdir ia-agent-service && cd ia-agent-service
   ```

2. Estrutura de pastas inicial:

   ```bash
   .
   â”œâ”€â”€ app/
   â”‚   â”œâ”€â”€ main.py
   â”‚   â”œâ”€â”€ api/routes.py
   â”‚   â”œâ”€â”€ agent/core.py
   â”‚   â”œâ”€â”€ agent/memory.py
   â”‚   â”œâ”€â”€ agent/context_loader.py
   â”‚   â”œâ”€â”€ agent/config.py
   â”‚   â”œâ”€â”€ db/database.py
   â”‚   â”œâ”€â”€ db/models.py
   â”‚   â”œâ”€â”€ llm/base.py
   â”‚   â”œâ”€â”€ llm/google_provider.py
   â”‚   â”œâ”€â”€ llm/openai_provider.py (placeholder)
   â”‚   â””â”€â”€ utils/formatter.py
   â”œâ”€â”€ alembic/
   â”œâ”€â”€ requirements.txt
   â”œâ”€â”€ .env
   â”œâ”€â”€ Dockerfile
   â””â”€â”€ docker-compose.yml
   ```

3. Criar ambiente virtual e instalar dependÃªncias:

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install fastapi uvicorn sqlalchemy alembic psycopg2-binary httpx python-dotenv google-generativeai notion-client
   pip freeze > requirements.txt
   ```

4. Dockerfile e docker-compose configurados para subir o FastAPI + PostgreSQL.

5. ConfiguraÃ§Ã£o do SDK do Google via `.env`:

   ```dotenv
   GOOGLE_API_KEY=xxxx
   LLM_PROVIDER=google
   NOTION_API_KEY=xxxx
   DATABASE_URL=postgresql://ia_user:secret@db:5432/ia_memory
   ```

---

### ğŸ”„ Fase 2: ImplementaÃ§Ã£o do Agente IA

**Objetivo:** Criar o agente principal com capacidade de:

* Receber pergunta via API,
* Consultar contexto (memÃ³ria + Notion),
* Gerar resposta com base na LLM (Google Gemini).

#### Etapas:

1. Definir interface `LLMProvider` (`base.py`) para suporte a mÃºltiplos provedores.
2. Implementar `GoogleProvider` com SDK oficial.
3. Utilizar o ambiente de testes interativo do SDK via Makersuite ou lib Python.
4. Criar `core.py` com lÃ³gica de integraÃ§Ã£o entre entrada â†’ memÃ³ria â†’ contexto â†’ resposta â†’ gravaÃ§Ã£o.

---

### ğŸ“Š Fase 3: Banco de Dados e MemÃ³ria Persistente

**Objetivo:** Permitir que o agente tenha memÃ³ria real e contextual.

#### Etapas:

1. Criar modelos ORM:

   * `mensagem`: pergunta, resposta, cliente\_id, timestamp
   * `sessao`: contexto geral

2. Configurar Alembic para migraÃ§Ãµes.

3. Desenvolver mÃ©todos em `memory.py` para manipulaÃ§Ã£o da memÃ³ria.

---

### ğŸ“š Fase 4: IntegraÃ§Ã£o com Notion para Contexto

**Objetivo:** Carregar documentos de conhecimento via Notion API.

#### Etapas:

1. Criar `context_loader.py`
2. Implementar funÃ§Ãµes para:

   * Buscar documentos por ID ou tag
   * Extrair e limpar texto
3. Integrar com `core.py` para compor o prompt final.

---

### ğŸš€ Fase 5: ExposiÃ§Ã£o da API REST

**Objetivo:** Permitir comunicaÃ§Ã£o externa com o serviÃ§o.

#### Endpoints principais:

* `POST /perguntar`
* `POST /evento/boasvindas`

#### Exemplo de entrada:

```json
{
  "mensagem": "Qual a garantia da linha A?",
  "cliente_id": 123
}
```

#### Exemplo de resposta:

```json
{
  "resposta": "A linha A possui 12 meses de garantia para uso urbano."
}
```

---

### ğŸ’¬ Fase 6: Personagem de Onboarding e Boas-Vindas

**Objetivo:** Criar uma experiÃªncia interativa com um personagem virtual (ex: Joelizinho).

#### Etapas:

1. Criar prompt base amigÃ¡vel.
2. Modelar entrada com contexto (ex: resumo de tickets).
3. Gerar resposta da IA com sugestÃµes prÃ¡ticas.
4. Exibir no frontend (balÃ£o/personagem).

---

### ğŸ”Œ Fase 7: Suporte a MÃºltiplas LLMs (ModularizaÃ§Ã£o)

**Objetivo:** Permitir alternÃ¢ncia entre diferentes provedores de IA.

#### Etapas:

1. Interface `LLMProvider` jÃ¡ definida.
2. Adicionar outros adaptadores (OpenAI, Ollama, Mistral).
3. Selecionar via `.env`:

   ```dotenv
   LLM_PROVIDER=google
   ```

---

### ğŸ§ª Fase 8: Testes Automatizados e Logs

**Objetivo:** Garantir qualidade e rastreabilidade.

#### Etapas:

* Escrever testes unitÃ¡rios (Pytest)
* Implementar logging estruturado
* Criar script de health check

---

### ğŸ Fase 9: PreparaÃ§Ã£o para ProduÃ§Ã£o

**Objetivo:** Garantir operaÃ§Ã£o estÃ¡vel em ambiente real.

#### Etapas:

* Configurar HTTPS e tokens de acesso
* Adicionar rate limiting
* Deploy via Docker/Kubernetes
* Pipeline CI/CD com GitHub Actions

---

### ğŸ‰ FinalizaÃ§Ã£o

Com esse planejamento, temos um serviÃ§o pronto para rodar isolado ou integrado, com suporte Ã  IA contextual, agentes com memÃ³ria, mÃºltiplas LLMs, e uma base sÃ³lida para escalar com seguranÃ§a e performance.
